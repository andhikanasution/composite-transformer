{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe050bcf-adaf-4a3e-9a10-cc4c6ee63f4c",
   "metadata": {},
   "source": [
    "# Testing: Composite Stress Dataset Loader\n",
    "\n",
    "This notebook tests the `CompositeStressDataset` class and ensures:\n",
    "- Time-series inputs and targets are loaded correctly.\n",
    "- Padding is applied as expected.\n",
    "- Input/target shapes are suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e194a1d-cdda-422f-84f9-068e047c749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = os.path.abspath(\"..\")  # Go up from /notebook\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "sys.path.append(src_path)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85362cfe-7b37-4fe0-93c7-727d5017d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/composite-transformer/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompositeStressDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofBristol/2. Data Science MSc/Modules/Data Science Project/composite_stress_prediction/src/dataset.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils_parsing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_all_data\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalisation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_stats, StandardScaler, apply_scaling\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from dataset import CompositeStressDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761320d-9663-4b50-828d-adb98ae77b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "input_csv_path = os.path.join(project_root, \"data\", \"IM78552_DATABASEInput.csv\")\n",
    "data_dir = os.path.join(project_root, \"data\", \"_CSV\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CompositeStressDataset(input_csv_path=input_csv_path, data_dir=data_dir, max_seq_len=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10176ab0-2092-4135-a2de-f2319f3b9c31",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "\n",
    "Letâ€™s check the size of the dataset and inspect a sample to confirm correct parsing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b079cd-8c94-48ac-8ac6-55c3c78cfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset size\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "\n",
    "# Preview first item\n",
    "x, y = dataset[0]\n",
    "print(\"Input shape:\", x.shape)   # Expected: [1800, 11]\n",
    "print(\"Target shape:\", y.shape)  # Expected: [1800, 6]\n",
    "\n",
    "# Show first few time steps\n",
    "print(\"\\nSample Input (first 3 timesteps):\\n\", x[:3])\n",
    "print(\"\\nSample Output (first 3 timesteps):\\n\", y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596e61f-4a2e-4317-a395-d02e01a990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few features from one sequence\n",
    "time_steps = range(x.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot input strain E1â€“E3 (columns 0â€“2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(time_steps, x[:, 0], label='E1')\n",
    "plt.plot(time_steps, x[:, 1], label='E2')\n",
    "plt.plot(time_steps, x[:, 2], label='E3')\n",
    "plt.title(\"Input Strain Components (E1â€“E3)\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot output stress S1â€“S3\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(time_steps, y[:, 0], label='S1')\n",
    "plt.plot(time_steps, y[:, 1], label='S2')\n",
    "plt.plot(time_steps, y[:, 2], label='S3')\n",
    "plt.title(\"Output Stress Components (S1â€“S3)\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d2687-eb97-4b38-94f3-53c6410062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for batch_x, batch_y in dataloader:\n",
    "    print(\"Batch input shape:\", batch_x.shape)   # [4, 1800, 11]\n",
    "    print(\"Batch target shape:\", batch_y.shape)  # [4, 1800, 6]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47319b9-e51d-403f-b40a-4ba405731574",
   "metadata": {},
   "source": [
    "## Additional Utility Function Tests (utils_parsing.py)\n",
    "\n",
    "This section explicitly tests each utility function from `utils_parsing.py` to ensure:\n",
    "- Metadata loading works and returns correct dimensions.\n",
    "- Time-series files are correctly parsed into strain and stress.\n",
    "- File mapping between index and path is built correctly.\n",
    "- Padding function behaves as expected.\n",
    "- All-in-one loader correctly builds padded input and output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3692d-0d2f-4c58-969d-1e9ca5bcf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_parsing import (\n",
    "    load_metadata,\n",
    "    load_time_series,\n",
    "    build_sample_mapping,\n",
    "    pad_sequence,\n",
    "    load_all_data\n",
    ")\n",
    "\n",
    "# Paths\n",
    "input_csv_path = \"../data/IM78552_DATABASEInput.csv\"\n",
    "sample_csv_path = \"../data/_CSV/IM78552_DATABASE_001.csv\"\n",
    "data_dir = \"../data/_CSV\"\n",
    "\n",
    "### 1. Test metadata loader\n",
    "metadata = load_metadata(input_csv_path)\n",
    "print(f\"[Metadata] Shape: {metadata.shape} | Sample:\\n\", metadata[:2])\n",
    "\n",
    "### 2. Test loading a sample time-series\n",
    "strain, stress = load_time_series(sample_csv_path)\n",
    "print(f\"\\n[Time-Series] Strain shape: {strain.shape}, Stress shape: {stress.shape}\")\n",
    "\n",
    "### 3. Test file mapping function\n",
    "mapping = build_sample_mapping(data_dir)\n",
    "print(f\"\\n[Mapping] Total mappings: {len(mapping)} | Sample entry:\\n\", list(mapping.items())[:1])\n",
    "\n",
    "### 4. Test padding function\n",
    "short_seq = strain[:10]\n",
    "padded_seq = pad_sequence(short_seq, max_len=15)\n",
    "print(f\"\\n[Padding] Original: {short_seq.shape}, Padded: {padded_seq.shape}\")\n",
    "\n",
    "### 5. Test full dataset loader (load only first 5 to save time here)\n",
    "inputs, targets = load_all_data(input_csv_path, data_dir, max_seq_len=1800)\n",
    "print(f\"\\n[Full Loader] Loaded sequences: {len(inputs)}\")\n",
    "print(f\"Sample input shape: {inputs[0].shape}, Sample target shape: {targets[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91680f-5db5-4684-b463-439947414850",
   "metadata": {},
   "source": [
    "## dataloader.py testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb95b6-26e6-4617-b02e-3327fa38d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "\n",
    "# Set your paths\n",
    "input_csv_path = \"../data/IM78552_DATABASEInput.csv\"\n",
    "data_dir = \"../data/_CSV\"\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = get_dataloader(input_csv_path, data_dir, batch_size=16)\n",
    "\n",
    "# Test loading a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Input batch shape:\", batch[0].shape)   # Expected: [B, T, 11]\n",
    "print(\"Target batch shape:\", batch[1].shape)  # Expected: [B, T, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3678f-8c86-4a17-8704-eba13d8df46a",
   "metadata": {},
   "source": [
    "## Dataset Class Test with Standardisation\n",
    "\n",
    "In this section, we test the full dataset pipeline with standardisation enabled. This includes:\n",
    "- Parsing all time-series and metadata files\n",
    "- Padding and stacking sequences\n",
    "- Applying Z-score normalisation to both inputs and targets\n",
    "- Wrapping everything in a PyTorch-compatible Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8006a-6d59-4a01-8f25-78d29df2f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CompositeStressDataset\n",
    "\n",
    "# Define paths\n",
    "input_csv_path = \"../data/IM78552_DATABASEInput.csv\"\n",
    "data_dir = \"../data/_CSV\"\n",
    "\n",
    "# Initialise dataset with standardisation\n",
    "dataset = CompositeStressDataset(input_csv_path, data_dir, max_seq_len=1800, scale=True)\n",
    "\n",
    "# Check total number of samples\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "\n",
    "# Inspect one sample\n",
    "x, y = dataset[0]\n",
    "print(\"Input shape:\", x.shape)    # Expected: [1800, 11]\n",
    "print(\"Target shape:\", y.shape)   # Expected: [1800, 6]\n",
    "\n",
    "# Preview input features (first 5 timesteps)\n",
    "print(\"\\n[Standardised Input Features Sample (first 5 steps)]:\")\n",
    "print(x[:5])\n",
    "\n",
    "# Preview target stress values\n",
    "print(\"\\n[Standardised Target Stress Sample (first 5 steps)]:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155cf7-fb34-4560-a982-3305efae01c5",
   "metadata": {},
   "source": [
    "### Sanity Check: Mean and Std of Scaled Data\n",
    "\n",
    "If standardisation worked correctly, the overall input and target features (stacked across time and samples) should be approximately zero-mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a9777-72ad-4a9f-8476-9ae1b407fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stack inputs and targets\n",
    "X_all = np.vstack([x.numpy() for x, _ in dataset])\n",
    "Y_all = np.vstack([y.numpy() for _, y in dataset])\n",
    "\n",
    "# Compute overall stats\n",
    "print(\"Input Feature Means (should be ~0):\\n\", np.round(X_all.mean(axis=0), 4))\n",
    "print(\"Input Feature Stds  (should be ~1):\\n\", np.round(X_all.std(axis=0), 4))\n",
    "\n",
    "print(\"\\nTarget Feature Means (should be ~0):\\n\", np.round(Y_all.mean(axis=0), 4))\n",
    "print(\"Target Feature Stds  (should be ~1):\\n\", np.round(Y_all.std(axis=0), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79ed55-45be-4d86-9de4-43c8e8cc7919",
   "metadata": {},
   "source": [
    "## Test: Strain Filtering and Minimum Length Handling\n",
    "\n",
    "This section validates the strain threshold filtering and ensures that:\n",
    "- All retained time steps in each sequence contain strain values within Â±`strain_threshold`.\n",
    "- Sequences shorter than `min_timesteps` after filtering are discarded.\n",
    "- The new parameters behave as expected when passed to `load_all_data`.\n",
    "\n",
    "Weâ€™ll log how many sequences were retained vs skipped, and manually inspect strain range compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c91f33-a39a-4927-9a22-c0fc25b39f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for filtering and loading\n",
    "strain_threshold = 0.05\n",
    "min_timesteps = 20\n",
    "max_seq_len = 1800\n",
    "\n",
    "from utils_parsing import load_all_data, build_sample_mapping\n",
    "\n",
    "# Count how many samples exist before filtering\n",
    "sample_mapping = build_sample_mapping(data_dir)\n",
    "total_samples_before = len(sample_mapping)\n",
    "\n",
    "# Load filtered dataset\n",
    "inputs_filtered, targets_filtered = load_all_data(\n",
    "    input_csv_path=input_csv_path,\n",
    "    data_dir=data_dir,\n",
    "    max_seq_len=max_seq_len,\n",
    "    strain_threshold=strain_threshold,\n",
    "    min_timesteps=min_timesteps\n",
    ")\n",
    "\n",
    "# Count how many samples survived filtering\n",
    "total_samples_after = len(inputs_filtered)\n",
    "\n",
    "# Report results\n",
    "print(f\"Total samples before filtering: {total_samples_before}\")\n",
    "print(f\"Total samples after filtering:  {total_samples_after}\")\n",
    "print(f\"Samples removed:                {total_samples_before - total_samples_after}\")\n",
    "print(f\"\\nExample input shape:  {inputs_filtered[0].shape}\")\n",
    "print(f\"Example target shape: {targets_filtered[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbb396-4334-4d43-a4ce-116d391d56db",
   "metadata": {},
   "source": [
    "### Verify Strain Range Compliance\n",
    "\n",
    "We now confirm that all retained strain values fall within Â±`strain_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53347f4-5965-4a12-9012-7015d2e200b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stack all loaded sequences to check value ranges (only pre-padding part)\n",
    "violations = 0\n",
    "total_time_steps = 0\n",
    "\n",
    "for i, input_seq in enumerate(inputs_filtered):\n",
    "    # Remove padded rows (assume padding = 0 and check if entire row is 0)\n",
    "    valid_rows = ~np.all(input_seq == 0, axis=1)\n",
    "    strain_data = input_seq[valid_rows][:, :6]  # Extract only E1â€“E6\n",
    "\n",
    "    total_time_steps += strain_data.shape[0]\n",
    "    if not np.all(np.abs(strain_data) <= strain_threshold):\n",
    "        print(f\"âš ï¸ Strain value out of bounds in sample {i}\")\n",
    "        violations += 1\n",
    "\n",
    "print(f\"Strain range verification complete.\")\n",
    "print(f\"Violations found: {violations} / {len(inputs_filtered)} samples\")\n",
    "print(f\"Total valid time steps checked: {total_time_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca6a14-b9fb-4a0c-aa24-3090ef0f8626",
   "metadata": {},
   "source": [
    "## Visualise Sample Strain Components After Filtering\n",
    "\n",
    "This plot helps us inspect how the strain components (E1â€“E6) evolve over time in individual sequences **after applying the Â±5% filtering**.\n",
    "\n",
    "Each plot shows all 6 strain components for a given sample. You should expect that:\n",
    "- All strain values remain within Â±0.05\n",
    "- Any sudden spikes or high-strain transitions have been removed\n",
    "- The remaining curves reflect the \"early-stage\" or elastic/plastic deformation regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67180b3a-82d4-4766-b3bf-f3d2d9fb2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_strain_components(sample_input, sample_id=None, threshold=None):\n",
    "    \"\"\"\n",
    "    Plot all 6 strain components (E1â€“E6) from a single sample.\n",
    "\n",
    "    Args:\n",
    "        sample_input (np.ndarray): Input sequence of shape [T, 11]\n",
    "        sample_id (int or str, optional): ID for labeling the plot\n",
    "        threshold (float, optional): Optional horizontal threshold lines\n",
    "    \"\"\"\n",
    "    time_steps = np.arange(sample_input.shape[0])\n",
    "    strain_data = sample_input[:, :6]  # E1â€“E6\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(6):\n",
    "        plt.plot(time_steps, strain_data[:, i], label=f'E{i+1}')\n",
    "    \n",
    "    if threshold:\n",
    "        plt.axhline(y=threshold, color='gray', linestyle='--', linewidth=1)\n",
    "        plt.axhline(y=-threshold, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "    title = f\"Strain Components Over Time â€” Sample {sample_id}\" if sample_id is not None else \"Strain Components Over Time\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Strain\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ðŸ” Plot a few filtered samples\n",
    "for i in [0, 1, 2]:\n",
    "    plot_strain_components(inputs_filtered[i], sample_id=i, threshold=strain_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08094de8-d612-48a1-b264-2111d8784d3a",
   "metadata": {},
   "source": [
    "## Test: Global vs. Selective Strain Thresholding\n",
    "\n",
    "We will now test both updated functions:\n",
    "- `load_time_series()`: which supports both global and per-component thresholds\n",
    "- `load_all_data()`: now accepts a `component_thresholds` dictionary\n",
    "\n",
    "### Goals:\n",
    "- Compare dataset sizes and input statistics using:\n",
    "  - A **global Â±5% threshold** (baseline)\n",
    "  - A **selective threshold** (Â±10% for E5â€“E6 only)\n",
    "- Ensure both methods return sequences of correct shape\n",
    "- Verify that per-component filtering is applied correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de07f8-140e-4199-b4b1-1af4fb53b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline test with global Â±5% filtering\n",
    "from utils_parsing import load_all_data\n",
    "\n",
    "baseline_inputs, baseline_targets = load_all_data(\n",
    "    input_csv_path=input_csv_path,\n",
    "    data_dir=data_dir,\n",
    "    max_seq_len=1800,\n",
    "    strain_threshold=0.05,  # Global threshold\n",
    "    min_timesteps=20,\n",
    "    component_thresholds=None  # Explicitly test the default path\n",
    ")\n",
    "\n",
    "print(\"Global Threshold Test (Â±5%)\")\n",
    "print(f\"Samples loaded: {len(baseline_inputs)}\")\n",
    "print(f\"Example shape: Input {baseline_inputs[0].shape}, Target {baseline_targets[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98f6f9-0d00-4e0f-919b-7cd000e109db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective threshold: apply Â±0.1 to E5 and E6, Â±0.05 to E1â€“E4\n",
    "custom_thresholds = {\n",
    "    0: 0.05,  # E1\n",
    "    1: 0.05,  # E2\n",
    "    2: 0.05,  # E3\n",
    "    3: 0.05,  # E4\n",
    "    4: 0.10,  # E5\n",
    "    5: 0.10   # E6\n",
    "}\n",
    "\n",
    "selective_inputs, selective_targets = load_all_data(\n",
    "    input_csv_path=input_csv_path,\n",
    "    data_dir=data_dir,\n",
    "    max_seq_len=1800,\n",
    "    min_timesteps=20,\n",
    "    component_thresholds=custom_thresholds\n",
    ")\n",
    "\n",
    "print(\"Selective Threshold Test (E5â€“E6 Â±10%, E1â€“E4 Â±5%)\")\n",
    "print(f\"Samples loaded: {len(selective_inputs)}\")\n",
    "print(f\"Example shape: Input {selective_inputs[0].shape}, Target {selective_targets[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd24c8-6399-404a-80ba-50629a1e8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Custom thresholds applied during loading\n",
    "component_thresholds = {\n",
    "    0: 0.05,  # E1\n",
    "    1: 0.05,  # E2\n",
    "    2: 0.05,  # E3\n",
    "    3: 0.05,  # E4\n",
    "    4: 0.10,  # E5\n",
    "    5: 0.10   # E6\n",
    "}\n",
    "\n",
    "# Track violations\n",
    "violations = []\n",
    "total_checked = 0\n",
    "\n",
    "# Loop through each sample\n",
    "for sample_idx, input_seq in enumerate(selective_inputs):\n",
    "    # Extract strain components (E1â€“E6 only)\n",
    "    strain = input_seq[:, :6]  # shape: [T, 6]\n",
    "\n",
    "    # Remove padded rows (assuming padding is zero)\n",
    "    valid_rows = ~np.all(input_seq == 0, axis=1)\n",
    "    strain = strain[valid_rows]\n",
    "\n",
    "    # Check each conditioned component\n",
    "    for comp_idx, threshold in component_thresholds.items():\n",
    "        exceeded = np.abs(strain[:, comp_idx]) > threshold\n",
    "        if np.any(exceeded):\n",
    "            violations.append((sample_idx, comp_idx, strain[exceeded][:5]))  # Log first 5 violations\n",
    "    total_checked += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"Checked {total_checked} samples with selective thresholds.\")\n",
    "print(f\"Violating samples found: {len(violations)}\")\n",
    "\n",
    "# Optionally preview the first few violations\n",
    "if violations:\n",
    "    print(\"\\nSample violations (sample index, component index, example values):\")\n",
    "    for v in violations[:5]:\n",
    "        print(v)\n",
    "else:\n",
    "    print(\"All strain values comply with their respective thresholds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb14ca-5a34-45a4-b2f3-eab5da666f47",
   "metadata": {},
   "source": [
    "### ðŸ§ª Sanity Check: Dataset Splitting Logic\n",
    "\n",
    "We verify that the train/val split logic correctly:\n",
    "- Loads mutually exclusive subsets\n",
    "- Maintains consistent splits across runs (via fixed seed)\n",
    "- Covers the full dataset without overlap\n",
    "\n",
    "This test uses:\n",
    "- `split='train'` and `split='val'` with the same `split_ratio` and `seed`\n",
    "- Set operations to check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69825b5-22cd-42a0-a365-0bdc05f913a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "\n",
    "# Constants\n",
    "INPUT_CSV = \"../data/IM78552_DATABASEInput.csv\"\n",
    "DATA_DIR = \"../data/_CSV\"\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 1800\n",
    "SPLIT_RATIO = 0.8\n",
    "SEED = 42\n",
    "\n",
    "# Load train and val loaders with same seed\n",
    "train_loader = get_dataloader(INPUT_CSV, DATA_DIR, MAX_SEQ_LEN, BATCH_SIZE, split='train', split_ratio=SPLIT_RATIO, seed=SEED)\n",
    "val_loader   = get_dataloader(INPUT_CSV, DATA_DIR, MAX_SEQ_LEN, BATCH_SIZE, split='val',   split_ratio=SPLIT_RATIO, seed=SEED)\n",
    "\n",
    "# Get sample indices from Subset objects\n",
    "train_indices = set(train_loader.dataset.indices)\n",
    "val_indices   = set(val_loader.dataset.indices)\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"Total samples     : {len(train_indices | val_indices)}\")\n",
    "print(f\"Train set size    : {len(train_indices)}\")\n",
    "print(f\"Validation set size: {len(val_indices)}\")\n",
    "print(f\"Overlap detected  : {len(train_indices & val_indices)}\")\n",
    "\n",
    "# Expect:\n",
    "# - Total = Full dataset\n",
    "# - Overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b959f1-50f4-4efa-863d-030eaf9e43a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
